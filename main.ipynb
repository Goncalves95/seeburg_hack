{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install all requeriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install sqlalchemy\n",
    "!pip3 install psycopg2  # For PostgreSQL\n",
    "!pip3 install pymysql   # For MySQL\n",
    "!pip3 install cx_Oracle # For Oracle \n",
    "!pip3 install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and conect the data bases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conect to source database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Define your connection string\n",
    "connection_string = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=Rahul\\MSSQLSERVER01;\"\n",
    "    \"DATABASE=ECC60jkl_HACK;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Use the connection to query the database with pandas\n",
    "source = pd.read_sql(\"SELECT * FROM dbo.A017\", connection)\n",
    "\n",
    "# Print the dataframe\n",
    "print(source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conect to target database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Define the connection string for the target database (Oracle EBS)\n",
    "target_connection_string = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=Rahul\\MSSQLSERVER01;\"\n",
    "    \"DATABASE=ORACLE_EBS_HACK;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "connection = pyodbc.connect(target_connection_string)\n",
    "\n",
    "# Use the connection to query the database with pandas\n",
    "source = pd.read_sql(\"SELECT * FROM APPLSYS_FND_FLEX_VALUES\", connection)\n",
    "\n",
    "# Print the dataframe\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Schema Infos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schema information for the Oracle EBS database\n",
    "target_cursor = target_connection.cursor()\n",
    "target_cursor.execute(\"SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS\")\n",
    "target_schema = target_cursor.fetchall()\n",
    "\n",
    "# Get the schema information for the ECC60jkl_HACK database\n",
    "source_cursor = connection.cursor()\n",
    "source_cursor.execute(\"SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS\")\n",
    "source_schema = source_cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_schema_dict = {}\n",
    "for table, column, data_type in target_schema:\n",
    "    print(f\"Target schema: {table}.{column} ({data_type})\")\n",
    "    if table not in target_schema_dict:\n",
    "        target_schema_dict[table] = []\n",
    "    target_schema_dict[table].append((column, data_type))\n",
    "\n",
    "print(f\"Target schema dict: {target_schema_dict}\")\n",
    "\n",
    "source_schema_dict = {}\n",
    "for table, column, data_type in source_schema:\n",
    "    print(f\"Source schema: {table}.{column} ({data_type})\")\n",
    "    if table not in source_schema_dict:\n",
    "        source_schema_dict[table] = []\n",
    "    source_schema_dict[table].append((column, data_type))\n",
    "\n",
    "print(f\"Source schema dict: {source_schema_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar tables and columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_tables = {}\n",
    "for target_table, target_columns in target_schema_dict.items():\n",
    "    for source_table, source_columns in source_schema_dict.items():\n",
    "        similarity = len(set(target_columns) & set(source_columns)) / len(set(target_columns) | set(source_columns))\n",
    "        if similarity > 0.5:  # adjust the threshold as needed\n",
    "            similar_tables[target_table] = source_table\n",
    "\n",
    "print(f\"Similar tables: {similar_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_columns = {}\n",
    "for target_table, target_columns in target_schema_dict.items():\n",
    "    for source_table, source_columns in source_schema_dict.items():\n",
    "        if target_table == source_table:  # only compare columns within the same table\n",
    "            for target_column_name in [col[1] for col in target_columns]:\n",
    "                for source_column_name in [col[1] for col in source_columns]:\n",
    "                    if target_column_name == source_column_name:\n",
    "                        similarity = 1.0  # exact match\n",
    "                    else:\n",
    "                        similarity = levenshtein_distance(target_column_name, source_column_name)\n",
    "                    if similarity > 0.5:  # adjust the threshold as needed\n",
    "                        similar_columns[(target_table, target_column_name)] = (source_table, source_column_name)\n",
    "\n",
    "print(f\"Similar columns: {similar_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install jellyfish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install jellyfish "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    return jellyfish.levenshtein_distance(s1, s2)\n",
    "\n",
    "column_mappings = {}\n",
    "for target_table, source_table in similar_tables.items():\n",
    "    target_columns = target_schema_dict[target_table]\n",
    "    source_columns = source_schema_dict[source_table]\n",
    "    for target_column, target_data_type in target_columns:\n",
    "        for source_column, source_data_type in source_columns:\n",
    "            if target_data_type == source_data_type:\n",
    "                similarity = 1 - levenshtein_distance(target_column, source_column) / max(len(target_column), len(source_column))\n",
    "                if similarity > 0.5:  # adjust the threshold as needed\n",
    "                    column_mappings[(target_table, target_column)] = (source_table, source_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refining mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import sqlite3\n",
    "\n",
    "# Define database connections\n",
    "target_connection = sqlite3.connect('target_database.db')\n",
    "source_connection = sqlite3.connect('source_database.db')\n",
    "\n",
    "target_cursor = target_connection.cursor()\n",
    "source_cursor = source_connection.cursor()\n",
    "\n",
    "# Get table names\n",
    "target_tables = [table[0] for table in target_cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")]\n",
    "source_tables = [table[0] for table in source_cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")]\n",
    "\n",
    "# Initialize column mappings\n",
    "column_mappings = {}\n",
    "refined_column_mappings = {}\n",
    "\n",
    "# Loop through tables and columns\n",
    "for target_table in target_tables:\n",
    "    for source_table in source_tables:\n",
    "        target_columns = [column[1] for column in target_cursor.execute(f\"PRAGMA table_info({target_table})\")]\n",
    "        source_columns = [column[1] for column in source_cursor.execute(f\"PRAGMA table_info({source_table})\")]\n",
    "        for target_column in target_columns:\n",
    "            for source_column in source_columns:\n",
    "                if target_column == source_column:\n",
    "                    column_mappings[(target_table, target_column)] = (source_table, source_column)\n",
    "\n",
    "# Refine column mappings\n",
    "for (target_table, target_column), (source_table, source_column) in column_mappings.items():\n",
    "    print(f\"Comparing {target_table}.{target_column} with {source_table}.{source_column}\")\n",
    "    target_data = pd.read_sql(f\"SELECT {target_column} FROM {target_table}\", target_connection)\n",
    "    source_data = pd.read_sql(f\"SELECT {source_column} FROM {source_table}\", source_connection)\n",
    "    \n",
    "    print(f\"Target data: {target_data.shape}\")\n",
    "    print(f\"Source data: {source_data.shape}\")\n",
    "    \n",
    "    # Check data distribution\n",
    "    target_dist = target_data[target_column].value_counts().index\n",
    "    source_dist = source_data[source_column].value_counts().index\n",
    "    similarity = len(set(target_dist) & set(source_dist)) / len(set(target_dist) | set(source_dist))\n",
    "    print(f\"Similarity: {similarity}\")\n",
    "    if similarity > 0.5:\n",
    "        # Check data types\n",
    "        if target_data[target_column].dtype == source_data[source_column].dtype:\n",
    "            print(f\"Data types match: {target_data[target_column].dtype}\")\n",
    "            # Check statistical properties\n",
    "            target_stats = target_data[target_column].describe()\n",
    "            source_stats = source_data[source_column].describe()\n",
    "            print(f\"Target stats: {target_stats}\")\n",
    "            print(f\"Source stats: {source_stats}\")\n",
    "            ks_test_p_value = stats.ks_2samp(target_data[target_column], source_data[source_column])[1]\n",
    "            print(f\"KS test p-value: {ks_test_p_value}\")\n",
    "            if ks_test_p_value > 0.05:\n",
    "                refined_column_mappings[(target_table, target_column)] = (source_table, source_column)\n",
    "                print(f\"Added mapping: {target_table}.{target_column} -> {source_table}.{source_column}\")\n",
    "        else:\n",
    "            print(f\"Data types don't match: {target_data[target_column].dtype} vs {source_data[source_column].dtype}\")\n",
    "    else:\n",
    "        print(f\"Similarity too low: {similarity}\")\n",
    "\n",
    "print(f\"Refined column mappings: {refined_column_mappings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to creat a table to display comparitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a table to display the comparisons\n",
    "comparisons = []\n",
    "for (target_table, target_column), (source_table, source_column) in refined_column_mappings.items():\n",
    "    target_data = pd.read_sql(f\"SELECT {target_column} FROM {target_table}\", target_connection)\n",
    "    source_data = pd.read_sql(f\"SELECT {source_column} FROM {source_table}\", source_connection)\n",
    "    \n",
    "    comparisons.append({\n",
    "        'Target Table': target_table,\n",
    "        'Target Column': target_column,\n",
    "        'Source Table': source_table,\n",
    "        'Source Column': source_column,\n",
    "        'Data Type': target_data[target_column].dtype,\n",
    "        'Similarity': len(set(target_data[target_column].value_counts().index) & set(source_data[source_column].value_counts().index)) / len(set(target_data[target_column].value_counts().index) | set(source_data[source_column].value_counts().index)),\n",
    "        'KS Test p-value': stats.ks_2samp(target_data[target_column], source_data[source_column])[1]\n",
    "    })\n",
    "\n",
    "comparisons_df = pd.DataFrame(comparisons)\n",
    "print(comparisons_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Mapping Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mapping_rules = []\n",
    "\n",
    "for mapping_rule in mapping_rules:\n",
    "    source_column = mapping_rule[\"S_Field\"]\n",
    "    target_column = mapping_rule[\"T_Field\"]\n",
    "    source_data_type = mapping_rule[\"S_Field_Type\"]\n",
    "    target_data_type = mapping_rule[\"T_Field_Type\"]\n",
    "\n",
    "    # Handle value mapping for each data type\n",
    "    if source_data_type == \"string\" and target_data_type == \"string\":\n",
    "        # Handle string value mapping (e.g., harmonizing terminology discrepancies)\n",
    "        value_mapping_rule = {\n",
    "            \"S_Field\": source_column,\n",
    "            \"T_Field\": target_column,\n",
    "            \"MAP_TYPE\": \"EQ\",  # or SQL for more complex mappings\n",
    "            \"Transformation Syntax\": \"LOWER(TRIM({}))\"  # example transformation syntax\n",
    "        }\n",
    "        value_mapping_rules.append(value_mapping_rule)\n",
    "\n",
    "    elif source_data_type == \"date\" and target_data_type == \"date\":\n",
    "        # Handle date value mapping (e.g., converting date formats)\n",
    "        value_mapping_rule = {\n",
    "            \"S_Field\": source_column,\n",
    "            \"T_Field\": target_column,\n",
    "            \"MAP_TYPE\": \"SQL\",\n",
    "            \"Transformation Syntax\": \"TO_DATE({}, 'YYYY-MM-DD')\"  # example transformation syntax\n",
    "        }\n",
    "        value_mapping_rules.append(value_mapping_rule)\n",
    "\n",
    "    elif source_data_type == \"numeric\" and target_data_type == \"numeric\":\n",
    "        # Handle numeric value mapping (e.g., converting decimal formats)\n",
    "        value_mapping_rule = {\n",
    "            \"S_Field\": source_column,\n",
    "            \"T_Field\": target_column,\n",
    "            \"MAP_TYPE\": \"EQ\",  # or SQL for more complex mappings\n",
    "            \"Transformation Syntax\": \"ROUND({}, 2)\"  # example transformation syntax\n",
    "        }\n",
    "        value_mapping_rules.append(value_mapping_rule)\n",
    "\n",
    "    else:\n",
    "        # Handle other data type combinations (e.g., boolean, timestamp, etc.)\n",
    "        pass\n",
    "\n",
    "print(f\"Value mapping rules: {value_mapping_rules}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
